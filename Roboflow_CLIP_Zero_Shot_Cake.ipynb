{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Roboflow-CLIP-Zero-Shot-Classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/KOSA-Big-Data_Vision/blob/main/Roboflow_CLIP_Zero_Shot_Cake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C76CDbIJi2DY"
      },
      "source": [
        "# How to use CLIP Zero-Shot on your own classificaiton dataset\n",
        "\n",
        "This notebook provides an example of how to benchmark CLIP's zero shot classification performance on your own classification dataset.\n",
        "\n",
        "[CLIP](https://openai.com/blog/clip/) is a new zero shot image classifier relased by OpenAI that has been trained on 400 million text/image pairs across the web. CLIP uses these learnings to make predicts based on a flexible span of possible classification categories.\n",
        "\n",
        "CLIP is zero shot, that means **no training is required**. \n",
        "\n",
        "Try it out on your own task here!\n",
        "\n",
        "Be sure to experiment with various text prompts to unlock the richness of CLIP's pretraining procedure.\n",
        "\n",
        "\n",
        "![Roboflow Wordmark](https://i.imgur.com/dcLNMhV.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOF3Feb7jrnu"
      },
      "source": [
        "# Download and Install CLIP Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyUIWjzOi23X",
        "outputId": "16560ad0-e9ac-4c55-a7a5-ee478f44715c"
      },
      "source": [
        "#installing some dependencies, CLIP was release in PyTorch\n",
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA version: 11.0\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu110\n",
            "  Downloading https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp37-cp37m-linux_x86_64.whl (1156.8 MB)\n",
            "\u001b[K     |███████████████████████         | 834.1 MB 1.5 MB/s eta 0:03:31tcmalloc: large alloc 1147494400 bytes == 0x55b2e2638000 @  0x7f8086156615 0x55b2a917002c 0x55b2a925017a 0x55b2a9172e4d 0x55b2a9264c0d 0x55b2a91e70d8 0x55b2a91e1c35 0x55b2a917473a 0x55b2a91e6f40 0x55b2a91e1c35 0x55b2a917473a 0x55b2a91e393b 0x55b2a9265a56 0x55b2a91e2fb3 0x55b2a9265a56 0x55b2a91e2fb3 0x55b2a9265a56 0x55b2a91e2fb3 0x55b2a9174b99 0x55b2a91b7e79 0x55b2a91737b2 0x55b2a91e6e65 0x55b2a91e1c35 0x55b2a917473a 0x55b2a91e393b 0x55b2a91e1c35 0x55b2a917473a 0x55b2a91e2b0e 0x55b2a917465a 0x55b2a91e2d67 0x55b2a91e1c35\n",
            "\u001b[K     |█████████████████████████████▏  | 1055.7 MB 1.3 MB/s eta 0:01:18tcmalloc: large alloc 1434370048 bytes == 0x55b326c8e000 @  0x7f8086156615 0x55b2a917002c 0x55b2a925017a 0x55b2a9172e4d 0x55b2a9264c0d 0x55b2a91e70d8 0x55b2a91e1c35 0x55b2a917473a 0x55b2a91e6f40 0x55b2a91e1c35 0x55b2a917473a 0x55b2a91e393b 0x55b2a9265a56 0x55b2a91e2fb3 0x55b2a9265a56 0x55b2a91e2fb3 0x55b2a9265a56 0x55b2a91e2fb3 0x55b2a9174b99 0x55b2a91b7e79 0x55b2a91737b2 0x55b2a91e6e65 0x55b2a91e1c35 0x55b2a917473a 0x55b2a91e393b 0x55b2a91e1c35 0x55b2a917473a 0x55b2a91e2b0e 0x55b2a917465a 0x55b2a91e2d67 0x55b2a91e1c35\n",
            "\u001b[K     |████████████████████████████████| 1156.7 MB 1.4 MB/s eta 0:00:01tcmalloc: large alloc 1445945344 bytes == 0x55b37c47a000 @  0x7f8086156615 0x55b2a917002c 0x55b2a925017a 0x55b2a9172e4d 0x55b2a9264c0d 0x55b2a91e70d8 0x55b2a91e1c35 0x55b2a917473a 0x55b2a91e2d67 0x55b2a91e1c35 0x55b2a917473a 0x55b2a91e2d67 0x55b2a91e1c35 0x55b2a917473a 0x55b2a91e2d67 0x55b2a91e1c35 0x55b2a917473a 0x55b2a91e2d67 0x55b2a91e1c35 0x55b2a917473a 0x55b2a91e2d67 0x55b2a917465a 0x55b2a91e2d67 0x55b2a91e1c35 0x55b2a917473a 0x55b2a91e393b 0x55b2a91e1c35 0x55b2a917473a 0x55b2a91e393b 0x55b2a91e1c35 0x55b2a9174dd1\n",
            "\u001b[K     |████████████████████████████████| 1156.8 MB 15 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu110\n",
            "  Downloading https://download.pytorch.org/whl/cu110/torchvision-0.8.2%2Bcu110-cp37-cp37m-linux_x86_64.whl (12.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.9 MB 158 kB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu110) (7.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Building wheels for collected packages: ftfy\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=2059d782222503a79dae0fc1c6ca7113b4aaeb5f8f537bf89198a4b3b432af80\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "Successfully built ftfy\n",
            "Installing collected packages: torch, torchvision, ftfy\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.7.1+cu110 which is incompatible.\u001b[0m\n",
            "Successfully installed ftfy-6.0.3 torch-1.7.1+cu110 torchvision-0.8.2+cu110\n",
            "Torch version: 1.7.1+cu110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HN7Q4rv5X1m",
        "outputId": "5845bbfb-b373-4dfc-dfe3-d252a2441e73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqN0UVpssA7J",
        "outputId": "d89ffffd-60d6-4cab-df94-d0a0cb97f65d"
      },
      "source": [
        "#clone the CLIP repository\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "%cd CLIP"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CLIP'...\n",
            "remote: Enumerating objects: 142, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 142 (delta 23), reused 33 (delta 15), pack-reused 91\u001b[K\n",
            "Receiving objects: 100% (142/142), 8.82 MiB | 22.52 MiB/s, done.\n",
            "Resolving deltas: 100% (63/63), done.\n",
            "/content/CLIP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwCkgL73rHE0"
      },
      "source": [
        "# Download Classification Data or Object Detection Data\n",
        "\n",
        "We will download the [public flowers classificaiton dataset](https://public.roboflow.com/classification/flowers_classification) from Roboflow. The data will come out as folders broken into train/valid/test splits and seperate folders for each class label.\n",
        "\n",
        "You can easily download your own dataset from Roboflow in this format, too.\n",
        "\n",
        "We made a conversion from object detection to CLIP text prompts in Roboflow, too, if you want to try that out.\n",
        "\n",
        "\n",
        "To get your data into Roboflow, follow the [Getting Started Guide](https://blog.roboflow.ai/getting-started-with-roboflow/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC1MFOK84m8N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okoqk8j57jzY"
      },
      "source": [
        "!mkdir cake"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iau0TS0t4nbD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39659349-6f48-469e-cc19-4cb0a36407ad"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfzMc8iC5CLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29b7f465-2de1-4172-9b40-3b64359ff2b3"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6EFKbgR4uSw"
      },
      "source": [
        "!cp /content/drive/MyDrive/Flowers_Classification.v3-augmented.clip.zip ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYm8wYM_47uM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d45bcca-2458-4a83-97ee-3676b1cdc539"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHHnCdsKrCVF"
      },
      "source": [
        "#download classification data\n",
        "#replace with your link\n",
        "!curl -L \"https://public.roboflow.com/ds/vPLCmk4Knv?key=tCrKLQNpTi\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hR06qNp5KnC"
      },
      "source": [
        "!unzip ./Flowers.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m7FYTSjCN46",
        "outputId": "30e260e5-c44c-447a-d57f-b98b123eb0c7"
      },
      "source": [
        "import os\n",
        "#our the classes and images we want to test are stored in folders in the test set\n",
        "class_names = os.listdir('./test/')\n",
        "class_names.remove('_tokenization.txt')\n",
        "class_names"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dandelion', 'daisy', 'cake']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB2q9ZUr55jx",
        "outputId": "fe6f4e94-e104-4256-bc34-37af093aea86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class_names"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dandelion', 'daisy', 'cake']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MTFi7196CNW",
        "outputId": "f89813f1-5554-4540-8264-4a3187b776aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CLIP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QELzUB7pnr-h",
        "outputId": "1b66b836-3ecc-4da7-dbce-80d62914d841"
      },
      "source": [
        "#we auto generate some example tokenizations in Roboflow but you should edit this file to try out your own prompts\n",
        "#CLIP gets a lot better with the right prompting!\n",
        "#be sure the tokenizations are in the same order as your class_names above!\n",
        "%cat ./test/_tokenization.txt"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "daisy\n",
            "dandelion\n",
            "cake"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPQxIGxXn8bR",
        "outputId": "d35e7c0d-d5a6-4beb-d16b-ed7ab8bebbbe"
      },
      "source": [
        "#edit your prompts as you see fit here\n",
        "%%writefile ./test/_tokenization.txt\n",
        "daisy\n",
        "dandelion\n",
        "cake"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ./test/_tokenization.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpXaqPH3oSyO"
      },
      "source": [
        "candidate_captions = []\n",
        "with open('./test/_tokenization.txt') as f:\n",
        "    candidate_captions = f.read().splitlines()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtf_jp586hMy",
        "outputId": "81cfa254-4bab-4537-a8a6-467be0f692f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CLIP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy7Usly56HIM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a32e029c-909d-489f-c929-d688479face9"
      },
      "source": [
        "%cd ./CLIP/"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: './CLIP/'\n",
            "/content/CLIP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMZJy1SduxiE"
      },
      "source": [
        "# Run CLIP inference on your classification dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAi7cvucnFPr",
        "outputId": "55dfd8cc-074c-4269-c6ad-457a6ee257d0"
      },
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "def argmax(iterable):\n",
        "    return max(enumerate(iterable), key=lambda x: x[1])[0]\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, transform = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "correct = []\n",
        "\n",
        "#define our target classificaitons, you can should experiment with these strings of text as you see fit, though, make sure they are in the same order as your class names above\n",
        "text = clip.tokenize(candidate_captions).to(device)\n",
        "\n",
        "for cls in class_names:\n",
        "    class_correct = []\n",
        "    test_imgs = glob.glob('./test/' + cls + '/*.jpg')\n",
        "    for img in test_imgs:\n",
        "        print(img)\n",
        "        image = transform(Image.open(img)).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            image_features = model.encode_image(image)\n",
        "            text_features = model.encode_text(text)\n",
        "            \n",
        "            logits_per_image, logits_per_text = model(image, text)\n",
        "            probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "            pred = class_names[argmax(list(probs)[0])]\n",
        "            print(pred)\n",
        "            if pred == cls:\n",
        "                correct.append(1)\n",
        "                class_correct.append(1)\n",
        "            else:\n",
        "                correct.append(0)\n",
        "                class_correct.append(0)\n",
        "    \n",
        "    print('accuracy on class ' + cls + ' is :' + str(sum(class_correct)/len(class_correct)))\n",
        "print('accuracy on all is : ' + str(sum(correct)/len(correct)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./test/dandelion/13916196427_50a611008f_jpg.rf.85e8c90b29e72b6f2eb7b3d321db8f73.jpg\n",
            "daisy\n",
            "./test/dandelion/9472854850_fc9e1db673_jpg.rf.56dff30fc81dda636a82985f8a1927d3.jpg\n",
            "daisy\n",
            "./test/dandelion/8083321316_f62ea76f72_n_jpg.rf.3e4740907c8bcf183a2afab6e9c25f14.jpg\n",
            "daisy\n",
            "./test/dandelion/15358221063_2c6e548e84_jpg.rf.f5be905e876255a3c7d2f760bb63fd48.jpg\n",
            "daisy\n",
            "./test/dandelion/34552250422_320900fd8e_n_jpg.rf.a9a6f2855e8c0dc156e4b0877fd0e8d4.jpg\n",
            "dandelion\n",
            "./test/dandelion/8797114213_103535743c_m_jpg.rf.132646214f7e9bcbdda402ab688ca868.jpg\n",
            "daisy\n",
            "./test/dandelion/13900486390_5a25785645_n_jpg.rf.6ca8083ee0bd606f480c84359984ca19.jpg\n",
            "daisy\n",
            "./test/dandelion/1297972485_33266a18d9_jpg.rf.27db91a578818f7158d6d5d98dbfdc8a.jpg\n",
            "daisy\n",
            "./test/dandelion/4510938552_6f7bae172a_n_jpg.rf.44e3121f52842ca7b65a91745b892f74.jpg\n",
            "daisy\n",
            "./test/dandelion/4489359360_09db62f825_jpg.rf.dd23d2557efa164171ccc721e6f6b275.jpg\n",
            "daisy\n",
            "./test/dandelion/14053173516_a00150a919_m_jpg.rf.db62016c6ebd3912dcc994dbe53edaee.jpg\n",
            "daisy\n",
            "./test/dandelion/510874382_f7e3435043_jpg.rf.b29f5681a3f5849d65b2efd17365006d.jpg\n",
            "daisy\n",
            "./test/dandelion/9853885425_4a82356f1d_m_jpg.rf.85f56b961ce357080355a10d99204715.jpg\n",
            "daisy\n",
            "./test/dandelion/8757650550_113d7af3bd_jpg.rf.15f92a268cac709e3670303db4a842a2.jpg\n",
            "daisy\n",
            "./test/dandelion/2478018280_1be353ca8c_m_jpg.rf.13e686968dc86acd0a5922e50bf0c30d.jpg\n",
            "daisy\n",
            "./test/dandelion/19600096066_67dc941042_jpg.rf.94d3a0e5858197d0cea6a34d1079ac20.jpg\n",
            "daisy\n",
            "./test/dandelion/34719559905_46ba779d79_n_jpg.rf.68254c0bb5877d9d4e758c3c3c87ab75.jpg\n",
            "daisy\n",
            "./test/dandelion/3688128868_031e7b53e1_n_jpg.rf.10b3b81237b2b346fcfe9bb064efe578.jpg\n",
            "daisy\n",
            "./test/dandelion/5674707921_1ffd141bab_n_jpg.rf.2596c5cd360fc35c47bc33d7b4e071df.jpg\n",
            "daisy\n",
            "./test/dandelion/461632542_0387557eff_jpg.rf.f66aef4fc0a966a83d804e9b819a8db3.jpg\n",
            "daisy\n",
            "./test/dandelion/17220096449_0e535989f0_n_jpg.rf.8db24b20eb779259f9480a46592ab911.jpg\n",
            "daisy\n",
            "./test/dandelion/6983113346_21551e1b52_n_jpg.rf.b3d0cb87aa97f08eb240a27eb0b63dab.jpg\n",
            "daisy\n",
            "./test/dandelion/4523862714_b41b459c88_jpg.rf.7e35d44b33f191b29aa5c9527f919fd7.jpg\n",
            "daisy\n",
            "./test/dandelion/5003160931_cf8cbb846f_jpg.rf.9b902eb51d049c6949732aad95d4eaf7.jpg\n",
            "daisy\n",
            "./test/dandelion/16096748028_7876887ab2_jpg.rf.268b74eb940038b4b12138f59cb68eea.jpg\n",
            "daisy\n",
            "./test/dandelion/3530500952_9f94fb8b9c_m_jpg.rf.ac207bfbba68d0683867b5314828a4b8.jpg\n",
            "daisy\n",
            "./test/dandelion/18183515403_13a9ca6d86_n_jpg.rf.bd063bad62c4378582b6433f3ce2fcb8.jpg\n",
            "daisy\n",
            "./test/dandelion/3554435478_1a7ab743e9_n_jpg.rf.07d5667ff8bfd641e5004f6b2270d4b6.jpg\n",
            "daisy\n",
            "./test/dandelion/8613502159_d9ea67ba63_jpg.rf.cf240be43aa3907c0e6d4b705cb89432.jpg\n",
            "daisy\n",
            "./test/dandelion/33877161494_05686b7f7a_n_jpg.rf.ed0b5bf52a84350b9492009ec9e5c7c8.jpg\n",
            "daisy\n",
            "./test/dandelion/9293460423_7fbb1e3c32_n_jpg.rf.bbf8ab2a4b71c1cdd95c839dd03dac73.jpg\n",
            "daisy\n",
            "./test/dandelion/5129135346_3fa8e804d8_n_jpg.rf.e088200b9397d327366f5ed3b24c75a5.jpg\n",
            "daisy\n",
            "./test/dandelion/34587720941_ccbbc420ec_n_jpg.rf.0adc605a005797e5ef3c26bd4c7078f9.jpg\n",
            "daisy\n",
            "./test/dandelion/11768468623_9399b5111b_n_jpg.rf.2237b890deba7d703e59bf44398efdb1.jpg\n",
            "daisy\n",
            "./test/dandelion/8720503800_cab5c62a34_jpg.rf.1d6d4f796a29224232c39a61a0d0b659.jpg\n",
            "daisy\n",
            "./test/dandelion/15002906952_cab2cb29cf_jpg.rf.78c84dee8f0d178cb3a5a5f6a41cb3f7.jpg\n",
            "daisy\n",
            "./test/dandelion/34339792440_8224ca420d_n_jpg.rf.28965bc70aeff840d1cbfcb0b1cb0125.jpg\n",
            "daisy\n",
            "./test/dandelion/8376558865_19c5cd6fd6_n_jpg.rf.ba46776fd6d0edac2feae7f2c3724689.jpg\n",
            "daisy\n",
            "./test/dandelion/3459346147_faffff51c7_n_jpg.rf.83b918b2adadb84bf878504c607e8582.jpg\n",
            "daisy\n",
            "./test/dandelion/1241011700_261ae180ca_jpg.rf.53eafd845f17fbf30ebd87b04312a4e8.jpg\n",
            "daisy\n",
            "./test/dandelion/149782934_21adaf4a21_jpg.rf.165c84fde51c7e6aa89067beb86868f9.jpg\n",
            "daisy\n",
            "./test/dandelion/8663932737_0a603ab718_n_jpg.rf.d2d2a30c1e07cd5b7a547c0aad17b090.jpg\n",
            "daisy\n",
            "./test/dandelion/14292205986_da230467ef_jpg.rf.41579775e68a4c026e860f6cc0b23fba.jpg\n",
            "daisy\n",
            "./test/dandelion/7465850028_cdfaae235a_n_jpg.rf.fcb7180ac6415e063f59216117fb7fe5.jpg\n",
            "daisy\n",
            "./test/dandelion/7196683612_6c4cf05b24_jpg.rf.2cfae8f98efa6f23b54e8506e81bf1f7.jpg\n",
            "daisy\n",
            "./test/dandelion/27186992702_449dfa54ef_n_jpg.rf.ce8053716b98ede68da54a50c4cb3ebe.jpg\n",
            "daisy\n",
            "./test/dandelion/13887031789_97437f246b_jpg.rf.bb669fcee9abb45aabf5352790b35ec8.jpg\n",
            "daisy\n",
            "./test/dandelion/141340262_ca2e576490_jpg.rf.a9e7a7e679798619924bbc5cade9f806.jpg\n",
            "daisy\n",
            "./test/dandelion/6954604340_d3223ed296_m_jpg.rf.3c98e222f1e58b2ef29779b4b5630d76.jpg\n",
            "daisy\n",
            "./test/dandelion/34700475225_fbc12d0834_n_jpg.rf.9e2f64d8f6f3f5c0a649fa0ed1022151.jpg\n",
            "daisy\n",
            "./test/dandelion/14728922673_99086a3818_n_jpg.rf.cd767a17fc6ead9cd589041de880a8a4.jpg\n",
            "daisy\n",
            "./test/dandelion/26004221274_74900d17e1_n_jpg.rf.42c1062912396fd89448f9984080c6a1.jpg\n",
            "daisy\n",
            "./test/dandelion/7197581386_8a51f1bb12_n_jpg.rf.587722330b3a0f634957ca4f1d46f1c3.jpg\n",
            "daisy\n",
            "./test/dandelion/15139657325_74031c44fc_jpg.rf.803ef4e8fd620413d4344d49d9b3e680.jpg\n",
            "daisy\n",
            "./test/dandelion/14093789753_f0f1acdb57_jpg.rf.28b44ee77efcb9ac8ae5c33280de9fac.jpg\n",
            "daisy\n",
            "./test/dandelion/3585220976_5acac92d1c_jpg.rf.937da2bac5815ba707c9d50da10418a3.jpg\n",
            "daisy\n",
            "./test/dandelion/7243478942_30bf542a2d_m_jpg.rf.793dc46138fe541f54ead96785a8ce86.jpg\n",
            "daisy\n",
            "./test/dandelion/4588529727_4a79c61577_jpg.rf.c364d40053a8642deb3fd410bade1fea.jpg\n",
            "daisy\n",
            "./test/dandelion/33850973214_c1b4000d9c_n_jpg.rf.3d799530874a6cbce1eb1ae336ec953c.jpg\n",
            "daisy\n",
            "./test/dandelion/5628296138_9031791fab_jpg.rf.83bdf72c3a00be6ada06aa67ca56e1db.jpg\n",
            "daisy\n",
            "./test/dandelion/4953240903_a121fba81f_m_jpg.rf.dfbd83e4564f94015af495b933f9bf05.jpg\n",
            "daisy\n",
            "./test/dandelion/14060367700_fe87e99b6a_m_jpg.rf.eaf8222cde370b2ebb4d6a1378ec07b1.jpg\n",
            "daisy\n",
            "./test/dandelion/15268682367_5a4512b29f_m_jpg.rf.214113aa658ff9a40c6383d597a81d9f.jpg\n",
            "daisy\n",
            "./test/dandelion/6994933428_307b092ce7_m_jpg.rf.62c09fbdd89ba85d3537f5d127236fe3.jpg\n",
            "daisy\n",
            "./test/dandelion/5446666484_365f3be83a_n_jpg.rf.b9e2045c5e6c3a4c7e7f97208c7a4610.jpg\n",
            "daisy\n",
            "./test/dandelion/5996421299_b9bf488c1a_n_jpg.rf.3c0de45f7ae465fa9f87a60211398777.jpg\n",
            "daisy\n",
            "./test/dandelion/3458770076_17ed3a1225_jpg.rf.0e914ae995f38a465599ddb92765420e.jpg\n",
            "daisy\n",
            "./test/dandelion/34540904752_ae86e5f6ce_n_jpg.rf.1c3398c3b75e6675151220a174ff95f2.jpg\n",
            "daisy\n",
            "./test/dandelion/2938040169_eb38581359_jpg.rf.4569e5c84f6eabb13537095f2eca9f3e.jpg\n",
            "daisy\n",
            "./test/dandelion/7243174412_d3628e4cc4_m_jpg.rf.3a171e63d844752eec452386e4362e4f.jpg\n",
            "daisy\n",
            "./test/dandelion/3998275481_651205e02d_jpg.rf.88b1566a4548a5bb7d98b224534fefa0.jpg\n",
            "daisy\n",
            "./test/dandelion/4151883194_e45505934d_n_jpg.rf.e6d750a2d2fa9c88acc0a4c24e8bb83b.jpg\n",
            "daisy\n",
            "./test/dandelion/15005530987_e13b328047_n_jpg.rf.65c0964bbcf134356e9b6598b6798682.jpg\n",
            "daisy\n",
            "./test/dandelion/3513200808_390f1d63a7_m_jpg.rf.73d45d87b1573a6e9bb7f3467e7d2cfc.jpg\n",
            "daisy\n",
            "./test/dandelion/3398195641_456872b48b_n_jpg.rf.e265ac40570c8bf38cb832950b9f349e.jpg\n",
            "daisy\n",
            "./test/dandelion/13471273823_4800ca8eec_jpg.rf.ba3ad39b915d39d0ee59f4a14beefd99.jpg\n",
            "daisy\n",
            "./test/dandelion/6400843175_ef07053f8f_m_jpg.rf.31fc1a6ef37c85f41af7f93c628e150a.jpg\n",
            "daisy\n",
            "./test/dandelion/17688233756_21879104c1_n_jpg.rf.2c602dd144b226c6eaec1c52a123a388.jpg\n",
            "daisy\n",
            "./test/dandelion/2401343175_d2a892cf25_n_jpg.rf.8f9bba257cc9d8fafd2ff4da8a173799.jpg\n",
            "daisy\n",
            "./test/dandelion/4552591312_02fe1dcc04_n_jpg.rf.ae80a313b268b1aa2d5ac5c380b953df.jpg\n",
            "daisy\n",
            "./test/dandelion/3517492544_0fd3ed6a66_m_jpg.rf.023d55ce980d42b59b62acce37f2ffe9.jpg\n",
            "daisy\n",
            "./test/dandelion/2392273474_a64cef0eaf_n_jpg.rf.3705fdaf3d656cbc2bf8997ffaceb28e.jpg\n",
            "daisy\n",
            "./test/dandelion/11595255065_d9550012fc_jpg.rf.c8a6f9eb14c54ff8942412c5716d0bd6.jpg\n",
            "daisy\n",
            "./test/dandelion/3491333876_e3fed43c0d_jpg.rf.e24ee16e80dbe63c8629959ee6b402b4.jpg\n",
            "daisy\n",
            "./test/dandelion/6132275522_ce46b33c33_m_jpg.rf.57b01e9c534a3210f229897712f497d0.jpg\n",
            "daisy\n",
            "./test/dandelion/34592557281_5f254b3a46_n_jpg.rf.00947e5096dbed151f08f5c84ea7ffe3.jpg\n",
            "daisy\n",
            "./test/dandelion/4134441089_c8c1e6132a_jpg.rf.edca5d6f4cc4dc7b15488f670eb5b65f.jpg\n",
            "daisy\n",
            "./test/dandelion/9152356642_06ae73113f_jpg.rf.db7b6ccb21e376a96d5597c5cbb8c11f.jpg\n",
            "daisy\n",
            "./test/dandelion/7280221020_98b473b20d_n_jpg.rf.f9666b6d9684240deb0c162fc29497d2.jpg\n",
            "daisy\n",
            "./test/dandelion/5033866477_a77cccba49_m_jpg.rf.c957f23b2e995138535b3b0b99eee48c.jpg\n",
            "daisy\n",
            "./test/dandelion/129019877_8eea2978ca_m_jpg.rf.fefbc3553fc43efbc15140a65fff928c.jpg\n",
            "daisy\n",
            "./test/dandelion/808239968_318722e4db_jpg.rf.8b3f341a8b6ab03f5f38d0f9d40acc75.jpg\n",
            "daisy\n",
            "./test/dandelion/8707349105_6d06b543b0_jpg.rf.48dc354aac3a4aee48ae0d55c9e1ee6b.jpg\n",
            "daisy\n",
            "./test/dandelion/4500964841_b1142b50fb_n_jpg.rf.2f8b5152ea3e62f140937a540a900150.jpg\n",
            "daisy\n",
            "./test/dandelion/16510864164_3afa8ac37f_jpg.rf.52b0b5101b854a14cea5da6996575484.jpg\n",
            "daisy\n",
            "./test/dandelion/16159487_3a6615a565_n_jpg.rf.6d473a1fe680a3e930f3ff28464c46a9.jpg\n",
            "daisy\n",
            "./test/dandelion/9595369280_dd88b61814_jpg.rf.dfa19b8bb9653602a78617083b36dca9.jpg\n",
            "daisy\n",
            "./test/dandelion/3393060921_2328b752f4_jpg.rf.cab8ffcf64f4ddcb03ff73944d3d42d2.jpg\n",
            "daisy\n",
            "./test/dandelion/5608832856_f5d49de778_jpg.rf.a8dc767f6b06720677e5976be3156a66.jpg\n",
            "daisy\n",
            "./test/dandelion/14376454225_a1de336c5b_jpg.rf.285a4f2bbf8c8ba3e0a20d9bf40af4b5.jpg\n",
            "daisy\n",
            "./test/dandelion/23414449869_ee849a80d4_jpg.rf.75bc6edba69719ef993bde64f403751a.jpg\n",
            "daisy\n",
            "./test/dandelion/1776290427_9d8d5be6ac_jpg.rf.2b03fbba6d0862557f82ce13e0d6c81d.jpg\n",
            "daisy\n",
            "./test/dandelion/16744522344_8d21b1530d_n_jpg.rf.234876facfaf654db754d97876625ced.jpg\n",
            "daisy\n",
            "./test/dandelion/34719957845_c929f480a3_n_jpg.rf.970ec525603d2eeb576ce2517d1bf6c7.jpg\n",
            "daisy\n",
            "./test/dandelion/7280222348_a87725ca77_jpg.rf.78a9c3e6b47349930d5c0ea7320649d4.jpg\n",
            "daisy\n",
            "accuracy on class dandelion is :0.009523809523809525\n",
            "./test/daisy/1374193928_a52320eafa_jpg.rf.e398f632151f3ccc5729bc89527d7614.jpg\n",
            "dandelion\n",
            "./test/daisy/5904946193_bd1eb1f39d_n_jpg.rf.30f7c73f16b310f3ea36ab88cca6965d.jpg\n",
            "dandelion\n",
            "./test/daisy/1354396826_2868631432_m_jpg.rf.409eee37613d16dbc71365cb5615327e.jpg\n",
            "dandelion\n",
            "./test/daisy/2632216904_274aa17433_jpg.rf.45300162290259cfe9a680945d85c3f4.jpg\n",
            "dandelion\n",
            "./test/daisy/16482676953_5296227d40_n_jpg.rf.7ec79fc5ef598518544d96863254c3be.jpg\n",
            "dandelion\n",
            "./test/daisy/33843240613_0b736f9896_n_jpg.rf.7a082f903d2cf0aaf4f42baf09519223.jpg\n",
            "daisy\n",
            "./test/daisy/33843400403_db00aa16b8_n_jpg.rf.64efb71797dc8096ab5fe372a8c3ce71.jpg\n",
            "dandelion\n",
            "./test/daisy/4117918318_3c8935289b_m_jpg.rf.c9dc5cde0a68b9354cf1712f62168b58.jpg\n",
            "dandelion\n",
            "./test/daisy/15760153042_a2a90e9da5_m_jpg.rf.697d305f979af39340e1b9c9d9ee8b0f.jpg\n",
            "daisy\n",
            "./test/daisy/14613443462_d4ed356201_jpg.rf.76462b3f0cb5ca5a0b4b7c9894feb91c.jpg\n",
            "dandelion\n",
            "./test/daisy/476856232_7c35952f40_n_jpg.rf.28b0b70acae8079b2d47f23e5eefbd86.jpg\n",
            "daisy\n",
            "./test/daisy/12193032636_b50ae7db35_n_jpg.rf.e6c4eeb71c56e793a0d85f6d979dbe20.jpg\n",
            "dandelion\n",
            "./test/daisy/7538403124_f2fc48750a_jpg.rf.39fc5a380d9e0292651207460311fd5d.jpg\n",
            "dandelion\n",
            "./test/daisy/18442919723_d1251d3e14_n_jpg.rf.63fc73ffb788b8bca6a8adb298e082a0.jpg\n",
            "dandelion\n",
            "./test/daisy/34542837641_10492bf600_n_jpg.rf.1fd640bbbafa6349eb6550a3df63e3a0.jpg\n",
            "dandelion\n",
            "./test/daisy/7416083788_fcb4c4f27e_n_jpg.rf.2d472cec430def057801f48e2b210c8c.jpg\n",
            "dandelion\n",
            "./test/daisy/4790631791_21e9648097_n_jpg.rf.1832d669b0ba791616c97a52df3521df.jpg\n",
            "dandelion\n",
            "./test/daisy/18684594849_7dd3634f5e_n_jpg.rf.a2b07018b0bd7da828eb929cbd0ce9dd.jpg\n",
            "dandelion\n",
            "./test/daisy/4565255237_9ba29c4d4e_n_jpg.rf.c6ae54e9181acdb486e9360ada21921a.jpg\n",
            "daisy\n",
            "./test/daisy/33891398493_e0b6d7f683_n_jpg.rf.37a6c4970cf5e3ba04879f2ae6abe678.jpg\n",
            "dandelion\n",
            "./test/daisy/14866200659_6462c723cb_m_jpg.rf.ca61b503fd16ed5f84c8f980bf260850.jpg\n",
            "dandelion\n",
            "./test/daisy/3463313493_9497aa47e5_n_jpg.rf.8116f374bc71c06b065c2ef82c0757b3.jpg\n",
            "daisy\n",
            "./test/daisy/34571214621_f655295459_n_jpg.rf.54272370193af45f39956b40fb692970.jpg\n",
            "dandelion\n",
            "./test/daisy/1392131677_116ec04751_jpg.rf.da99f9bb579ee96dc1728545ca519d6f.jpg\n",
            "dandelion\n",
            "./test/daisy/2579018590_74359dcf1a_m_jpg.rf.09c66018bd90f02891f966ce3d0c4c47.jpg\n",
            "dandelion\n",
            "./test/daisy/4563059851_45a9d21a75_jpg.rf.322b95999c283d3b3aa41ae741838ce5.jpg\n",
            "daisy\n",
            "./test/daisy/519880292_7a3a6c6b69_jpg.rf.4329ebaba670fde97cec43b151764ae3.jpg\n",
            "dandelion\n",
            "./test/daisy/8709535323_a6bea3e43f_jpg.rf.0811cb3a0ef1b6eed5a91fb6f31c9a01.jpg\n",
            "dandelion\n",
            "./test/daisy/33855966243_01b2486428_n_jpg.rf.5f9f2886770c4b36b6231c2aa92c12cb.jpg\n",
            "dandelion\n",
            "./test/daisy/476857510_d2b30175de_n_jpg.rf.40ff83ae9c6f996b11149eaf1eafcc2e.jpg\n",
            "daisy\n",
            "./test/daisy/1342002397_9503c97b49_jpg.rf.8fe6bdd23186b70f089bb0c5b89d314e.jpg\n",
            "dandelion\n",
            "./test/daisy/16025261368_911703a536_n_jpg.rf.ecd6ca15230613fb42fb7de502c0deca.jpg\n",
            "dandelion\n",
            "./test/daisy/10466290366_cc72e33532_jpg.rf.6ddc91cd5d4a6a683e567ccb37e5a089.jpg\n",
            "daisy\n",
            "./test/daisy/22873310415_3a5674ec10_m_jpg.rf.481813ecfd025260ce55f8a383eef955.jpg\n",
            "dandelion\n",
            "./test/daisy/15813862117_dedcd1c56f_m_jpg.rf.81321871cfb8bfb90da77972a8bd257e.jpg\n",
            "dandelion\n",
            "./test/daisy/5905504340_1d60fa9611_n_jpg.rf.c12deb4c8d87dd57f34aa81eaefe809f.jpg\n",
            "dandelion\n",
            "./test/daisy/14707111433_cce08ee007_jpg.rf.5115333810dba8244f35512c99703520.jpg\n",
            "dandelion\n",
            "./test/daisy/14564545365_1f1d267bf1_n_jpg.rf.b848a44633a90175f6df714901554324.jpg\n",
            "dandelion\n",
            "./test/daisy/33810542134_a493f19a71_n_jpg.rf.1f84faf9a3e2b1dfd1a0db12829b6eb7.jpg\n",
            "cake\n",
            "./test/daisy/2482982436_a2145359e0_n_jpg.rf.fd93adb01dd4053a500ec796e81d6187.jpg\n",
            "dandelion\n",
            "./test/daisy/2578695910_5ab8ee17c1_n_jpg.rf.f6a916f0f067f27b7b8501051db99c48.jpg\n",
            "dandelion\n",
            "./test/daisy/22244161124_53e457bb66_n_jpg.rf.0647dd5bf7e8b1493bf4ab1aabe2df3e.jpg\n",
            "dandelion\n",
            "./test/daisy/10466558316_a7198b87e2_jpg.rf.7acf642b94dc98daa49482a12994ac4c.jpg\n",
            "dandelion\n",
            "./test/daisy/4858518329_7563eb0baa_m_jpg.rf.87b1251614bb568f499b6af22e3347ab.jpg\n",
            "dandelion\n",
            "./test/daisy/5765646947_82e95a9cc9_n_jpg.rf.81b496130743dfd52fce21f5e2dd9570.jpg\n",
            "dandelion\n",
            "./test/daisy/6136947177_47ff445eb4_n_jpg.rf.c11557c9c68f27732233f8837c47ff04.jpg\n",
            "dandelion\n",
            "./test/daisy/2520369272_1dcdb5a892_m_jpg.rf.497ae682965d0b1bd4089c348664664b.jpg\n",
            "dandelion\n",
            "./test/daisy/14907815010_bff495449f_jpg.rf.8ed410efb31aa282c8fb6002e36348e2.jpg\n",
            "dandelion\n",
            "./test/daisy/4482623536_b9fb5ae41f_n_jpg.rf.813cac3ea9f4432e19bf380c747867eb.jpg\n",
            "dandelion\n",
            "./test/daisy/14591326135_930703dbed_m_jpg.rf.10cc8279dba4fb0032a954596fbe99a9.jpg\n",
            "dandelion\n",
            "./test/daisy/33814092924_b23d019011_n_jpg.rf.dbf071a3be1a8b3dcc44b5331d102482.jpg\n",
            "dandelion\n",
            "./test/daisy/33879354664_615c72773d_n_jpg.rf.60764f87614dfb0231f4cc2de3767e6d.jpg\n",
            "dandelion\n",
            "./test/daisy/6776075110_1ea7a09dd4_n_jpg.rf.6e2cedfbd3d2732699dfd7f53a967866.jpg\n",
            "dandelion\n",
            "./test/daisy/488202750_c420cbce61_jpg.rf.ebf7d924209a96639fffea69bd279a8c.jpg\n",
            "dandelion\n",
            "./test/daisy/7066602021_2647457985_m_jpg.rf.29bb36e3cd303b49067673e6000954d1.jpg\n",
            "dandelion\n",
            "./test/daisy/18622672908_eab6dc9140_n_jpg.rf.89def480591c1c229e7e416383dadbf3.jpg\n",
            "dandelion\n",
            "./test/daisy/2621723097_736febb4a4_n_jpg.rf.3a6955a230ab66a0f09bf833bae5761b.jpg\n",
            "dandelion\n",
            "./test/daisy/3504430338_77d6a7fab4_n_jpg.rf.4a993cefe2d15b8f964ab28bf7606c85.jpg\n",
            "daisy\n",
            "./test/daisy/6089825811_80f253fbe1_jpg.rf.edcaebb528c2b993324dc3ead633f7b9.jpg\n",
            "dandelion\n",
            "./test/daisy/6299498346_b9774b6500_jpg.rf.f88a6eddc7fd755ce88201e09042e243.jpg\n",
            "dandelion\n",
            "./test/daisy/34562146951_cf3d2a627c_n_jpg.rf.abd65a51f37ada4fc322c6cad6613e02.jpg\n",
            "dandelion\n",
            "./test/daisy/9345273630_af3550031d_jpg.rf.3a2b558ca9c965532e017fa059d0bd14.jpg\n",
            "daisy\n",
            "./test/daisy/33874126263_3f6f965784_n_jpg.rf.140df6339c7cfdd085f75b7ab7b802f4.jpg\n",
            "dandelion\n",
            "./test/daisy/34695914906_961f92ffcd_n_jpg.rf.de50f4cd24e39e40099bb3339d6bb101.jpg\n",
            "dandelion\n",
            "./test/daisy/5896110423_e084b33401_n_jpg.rf.e39901a7d317219e85ec14794521d040.jpg\n",
            "daisy\n",
            "./test/daisy/14399435971_ea5868c792_jpg.rf.89b733453e10d461d440acc99b7ef6d1.jpg\n",
            "dandelion\n",
            "./test/daisy/14333681205_a07c9f1752_m_jpg.rf.6ff96d9fe33f0bd19a18425f32d470b1.jpg\n",
            "dandelion\n",
            "./test/daisy/16737503507_431768a927_jpg.rf.b2631742878b5c60bbb3eb7824623b55.jpg\n",
            "dandelion\n",
            "./test/daisy/5435513198_90ce39f1aa_n_jpg.rf.2dd15803ebf6cc6e5500274f0c21bb14.jpg\n",
            "daisy\n",
            "./test/daisy/6480809573_76a0074b69_n_jpg.rf.c91f24e489b548f07ba35d9e1bb034eb.jpg\n",
            "daisy\n",
            "./test/daisy/3699235066_fc09a02dfe_m_jpg.rf.083993c3ebd5cfdb89aef2abc23535fa.jpg\n",
            "daisy\n",
            "./test/daisy/14569895116_32f0dcb0f9_jpg.rf.73199cb0ad1cb9aa37d36b16a3c9b46c.jpg\n",
            "dandelion\n",
            "./test/daisy/9221345475_67735dbf4f_n_jpg.rf.0b725c9a5e7b3fa7e186056184b4ced4.jpg\n",
            "dandelion\n",
            "./test/daisy/754248840_95092de274_jpg.rf.5c15a2fc56cb948559bf57dbf87c69d6.jpg\n",
            "daisy\n",
            "./test/daisy/6207492986_0ff91f3296_jpg.rf.03fc13bc6c049b950ab318b108e0283d.jpg\n",
            "daisy\n",
            "./test/daisy/2877860110_a842f8b14a_m_jpg.rf.3534cdaf6c444986f34a010349914cf5.jpg\n",
            "dandelion\n",
            "./test/daisy/8322526877_95d1c0f8bc_n_jpg.rf.8f53aedefadc0395888c297925266102.jpg\n",
            "dandelion\n",
            "accuracy on class daisy is :0.2077922077922078\n",
            "./test/cake/cake1-jpg.jpg\n",
            "cake\n",
            "./test/cake/cake2-jpg.jpg\n",
            "cake\n",
            "accuracy on class cake is :1.0\n",
            "accuracy on all is : 0.10326086956521739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yrqnKyUofrb"
      },
      "source": [
        "#Hope you enjoyed!\n",
        "#As always, happy inferencing\n",
        "#Roboflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beM5N-2iudEk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}